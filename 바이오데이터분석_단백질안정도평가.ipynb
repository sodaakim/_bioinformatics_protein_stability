{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3yTwbhLMQNx",
        "outputId": "cd2f3aae-3a1a-4a47-c104-fdfd03fe427e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'iFeature'...\n",
            "remote: Enumerating objects: 322, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 322 (delta 40), reused 33 (delta 29), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (322/322), 6.72 MiB | 21.45 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf iFeature\n",
        "!git clone https://github.com/Superzchen/iFeature.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn_zli9Cv8VO",
        "outputId": "afb147e3-3c06-420e-f323-3c6a15aece73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "biodata_txt = '/content/drive/My Drive/Testdata/biodata.txt'\n",
        "\n",
        "\n",
        "# read txt file\n",
        "# ----------------------------------------------------\n",
        "\"\"\"\n",
        "# Load and get label in data : biodata_txt\n",
        "with open(biodata_txt, 'r') as file:\n",
        "    data = [line.split() for line in file.read().splitlines()]\n",
        "\n",
        "# Split sequences and labels\n",
        "GeneNames = [item[0] for item in data]\n",
        "proteinIDs = [item[1] for item in data]\n",
        "sequences = [item[2] for item in data]\n",
        "labels = [1 if item[3] == 'Stable' else 0 for item in data]\n",
        "\"\"\"\n",
        "\n",
        "# txt file 읽기\n",
        "# ----------------------------------------------------\n",
        "file = open(biodata_txt, \"r\")\n",
        "data = file.read()\n",
        "data_split = data.split()\n",
        "\n",
        "list_Amino = []\n",
        "list_Stable = []\n",
        "\n",
        "for i in range(0, len(data_split)):\n",
        "    if i%4 == 2:\n",
        "        list_Amino.append(data_split[i])\n",
        "    if i%4 == 3:\n",
        "        list_Stable.append(data_split[i])\n",
        "\n",
        "\n",
        "# 인덱스 지우기\n",
        "del list_Amino[0]\n",
        "del list_Stable[0]\n",
        "\n",
        "\n",
        "# 뒤집은 데이터 추가\n",
        "# ----------------------------------------------------\n",
        "# for AC in range(len(list_Amino)) :\n",
        "#   list_Amino.append(list_Amino[AC][::-1])\n",
        "\n",
        "# for stability in range(len(list_Stable)) :\n",
        "#   list_Stable.append(list_Stable[stability])\n",
        "\n",
        "# ----------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEdPKFPPnBi5"
      },
      "source": [
        "특징 추가 Model (iFeature **csv**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRh1UQiTyQft"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "AAC_features_csv = '/content/drive/My Drive/Testdata/AAC_features_model.csv'\n",
        "CTDC_features_csv = '/content/drive/My Drive/Testdata/CTDC_features_model.csv'\n",
        "\n",
        "model_feature_files = [ AAC_features_csv, CTDC_features_csv]\n",
        "\n",
        "model_features_list = []\n",
        "\n",
        "for feature_file in model_feature_files:\n",
        "    model_feature_list = []\n",
        "\n",
        "    # Open the file\n",
        "    with open(feature_file, 'r') as file:\n",
        "        # Create CSV reader object\n",
        "        csv_reader = csv.reader(file, delimiter='\\t')\n",
        "        next(csv_reader)\n",
        "\n",
        "        # Add each row to the list\n",
        "        for row in csv_reader:\n",
        "            model_feature_list.append([float(x) for x in row[1:]])\n",
        "\n",
        "    model_features_list.append(model_feature_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJ0yLfnnFwo"
      },
      "source": [
        "모델 : NNC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c-OL-4SzYKB",
        "outputId": "96c63eeb-58f0-4e4e-dd01-ac6c46b01a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5nmWlZLOohq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "\n",
        "import os\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "# 아미노산 서열의 최대 길이 뽑기\n",
        "maxlen = 0\n",
        "for str in list_Amino:\n",
        "    if maxlen < len(str):\n",
        "        print(str)\n",
        "        maxlen = len(str)\n",
        "print(maxlen)\n",
        "\n",
        "maxlen = 1653\n",
        "\n",
        "# 정규화 : 모두 길이를 일정하게 맞춤\n",
        "for i in range(0, len(list_Amino)):\n",
        "    while maxlen > len(list_Amino[i]):\n",
        "        list_Amino[i] += '0'\n",
        "\n",
        "# ---------------------------------------\n",
        "xdot = []\n",
        "x = []\n",
        "count = 0\n",
        "for st in list_Amino:\n",
        "    for letter in st:\n",
        "        xinit = ord(letter) - 64\n",
        "        if xinit == -16:\n",
        "            xinit = 0\n",
        "\n",
        "        xinit = float(xinit)/26\n",
        "        xdot.append(xinit)\n",
        "    x.append(xdot)\n",
        "    xdot = []\n",
        "    count += 1\n",
        "    if count % 100 == 0:\n",
        "        print(count)\n",
        "\n",
        "x = np.array(x)\n",
        "x = torch.tensor(x)\n",
        "\n",
        "# ---------------------------------------\n",
        "\n",
        "y = []\n",
        "for st in list_Stable:\n",
        "    if st == \"Stable\":\n",
        "        y.append(1)\n",
        "    else:\n",
        "        y.append(0)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "\n",
        "x = x.cuda()\n",
        "y = y.cuda()\n",
        "\n",
        "x = x.float()\n",
        "y = y.int()\n",
        "\n",
        "file.close()\n",
        "\n",
        "# features 변수에 저장된 넘파이 배열을 이용하여 모델 학습\n",
        "\n",
        "# Preprocessing and data loading part would be the same until features loading\n",
        "# IFeature\n",
        "# ---------------------------------------\n",
        "# features를 numpy array로 변환하고, torch tensor로 변환\n",
        "\n",
        "\n",
        "# AAC features를 numpy array로 변환하고, torch tensor로 변환\n",
        "AAC_features = np.array(model_features_list[0]).astype(np.float)\n",
        "AAC_features = torch.tensor(AAC_features).float().to(device)\n",
        "\n",
        "# CTDC features를 numpy array로 변환하고, torch tensor로 변환\n",
        "CTDC_features = np.array(model_features_list[1]).astype(np.float)\n",
        "CTDC_features = torch.tensor(CTDC_features).float().to(device)\n",
        "\n",
        "# # Concatenate the original features with AAC and CTDC features\n",
        "x = torch.cat((x, AAC_features), dim=1)\n",
        "x = torch.cat((x, CTDC_features), dim=1)\n",
        "\n",
        "inputsize = maxlen + AAC_features.shape[1] + CTDC_features.shape[1]\n",
        "\n",
        "# inputsize = 1653\n",
        "# ---------------------------------------\n",
        "\n",
        "class NNC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(inputsize, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for inn, (train, label) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # 예측(prediction)과 손실(loss) 계산\n",
        "        pred = model(train)\n",
        "\n",
        "        label = label.unsqueeze(1)\n",
        "        label = label.float()\n",
        "\n",
        "        sig = nn.Sigmoid()\n",
        "\n",
        "        pred = sig(pred)\n",
        "\n",
        "        loss = loss_fn(pred, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = []\n",
        "\n",
        "        for ien in range(len(label)):\n",
        "            output = 0\n",
        "            if pred[ien] > 0.5:\n",
        "                output = 1\n",
        "            else:\n",
        "                output = 0\n",
        "            if label[ien] == output:\n",
        "                acc.append(1)\n",
        "            else:\n",
        "                acc.append(0)\n",
        "\n",
        "        acc = sum(acc) / len(acc)\n",
        "\n",
        "        if inn % 200 == 0:\n",
        "\n",
        "            loss, current = loss.item(), (inn + 1) * len(train)\n",
        "            print(f\"loss: {loss:>7f} acc: {acc:>5f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "AAC_features_test_csv = '/content/drive/My Drive/Testdata/AAC_features_test.csv'\n",
        "CTDC_features_test_csv = '/content/drive/My Drive/Testdata/CTDC_features_test.csv'\n",
        "\n",
        "test_feature_files = [ AAC_features_test_csv, CTDC_features_test_csv ]\n",
        "# CTDC_features_test_csv\n",
        "# test_feature_files = []\n",
        "\n",
        "test_features_list = []\n",
        "for feature_file in test_feature_files:\n",
        "    test_feature_list = []\n",
        "\n",
        "    # Open the file\n",
        "    with open(feature_file, 'r') as file:\n",
        "        # Create CSV reader object\n",
        "        csv_reader = csv.reader(file, delimiter='\\t')\n",
        "        next(csv_reader)\n",
        "\n",
        "        # Add each row to the list\n",
        "        for row in csv_reader:\n",
        "            test_feature_list.append(row[1:])\n",
        "\n",
        "    test_features_list.append(test_feature_list)\n",
        "\n",
        "\n",
        "# test_model 함수 정의\n",
        "def test_model(test_sequences, features_list, ground_truth_labels):\n",
        "    # Preprocessing the sequences similar to the training sequences\n",
        "    xdott = []\n",
        "    x_test = []  # Initialize x_test here\n",
        "    for i in range(len(test_sequences)):\n",
        "        st = test_sequences[i]\n",
        "        for letter in st:\n",
        "            xinit = ord(letter) - 64\n",
        "            if xinit == -16:\n",
        "                xinit = 0\n",
        "            xinit = float(xinit)/26\n",
        "            xdott.append(xinit)\n",
        "        # Append features for the sequence from each feature list\n",
        "        for feature in features_list:\n",
        "            feature_values = [float(val) for val in feature[i]]\n",
        "            xdott.extend(feature_values)\n",
        "        x_test.append(xdott)\n",
        "        xdott = []\n",
        "\n",
        "    # Convert the list to a tensor\n",
        "    x_test = torch.tensor(x_test).float().to(device)\n",
        "\n",
        "    y_test = []\n",
        "    for st in ground_truth_labels:\n",
        "        if st == \"Stable\":\n",
        "            y_test.append(1)\n",
        "        else:\n",
        "            y_test.append(0)\n",
        "    y_test = torch.tensor(y_test).int().to(device)\n",
        "\n",
        "    # TensorDataset과 DataLoader로 변환합니다.\n",
        "    test_data = TensorDataset(x_test, y_test)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=50)  # 적절한 배치 크기를 선택합니다.\n",
        "\n",
        "    # 예측 결과를 저장할 리스트를 초기화합니다.\n",
        "    all_probabilities = []\n",
        "\n",
        "    # 양성 클래스를 1, 음성 클래스를 0으로 매핑한 ground truth labels 생성\n",
        "    binary_labels = [1 if label == 'Stable' else 0 for label in ground_truth_labels]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mymodel.eval()\n",
        "        for i, (inputs, labels) in enumerate(test_dataloader):\n",
        "            output = mymodel(inputs)\n",
        "            sig = nn.Sigmoid()\n",
        "            output = sig(output)\n",
        "\n",
        "            # 예측 확률을 리스트에 추가합니다.\n",
        "            batch_probabilities = [val[0] for val in output.tolist()]\n",
        "            all_probabilities.extend(batch_probabilities)\n",
        "\n",
        "    # 확률의 중간값을 계산합니다.\n",
        "    median_prob = np.median(all_probabilities)\n",
        "\n",
        "    print(\"===================================\")\n",
        "    print(\"Median Probability:\", median_prob)\n",
        "\n",
        "    # ROC 곡선을 기반으로 임계값을 설정합니다.\n",
        "    fpr, tpr, thresholds = roc_curve(binary_labels, all_probabilities)\n",
        "    threshold_idx = np.argmax(tpr - fpr)\n",
        "    threshold = thresholds[threshold_idx]\n",
        "\n",
        "    #threshold = median_prob\n",
        "    print(threshold)\n",
        "\n",
        "    # 예측 결과를 임계값을 기준으로 설정하여 리스트에 추가합니다.\n",
        "    all_predictions = [\"Stable\" if val > threshold else \"Degradable\" for val in all_probabilities]\n",
        "\n",
        "    # 정확도 계산\n",
        "    correct_predictions = sum([1 if prediction == truth else 0 for prediction, truth in zip(all_predictions, ground_truth_labels)])\n",
        "    accuracy = correct_predictions / len(ground_truth_labels)\n",
        "\n",
        "    # TPR 및 FPR 계산\n",
        "    fpr, tpr, thresholds = roc_curve(binary_labels, all_probabilities)\n",
        "\n",
        "    # AUC 계산\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"AUC:\", auc_score)\n",
        "\n",
        "    return all_predictions, accuracy, all_probabilities, y_test\n",
        "\n",
        "# 드라이브 마운트\n",
        "drive.mount('/content/drive')\n",
        "testdata_txt = '/content/drive/My Drive/Testdata/testdata.txt'\n",
        "\n",
        "# test data 파일 읽기\n",
        "file = open(testdata_txt, \"r\")\n",
        "data = file.read()\n",
        "data_split = data.split()\n",
        "\n",
        "test_sequences = []\n",
        "ground_truth_labels = []\n",
        "\n",
        "for i in range(0, len(data_split)):\n",
        "    if i%4 == 2:\n",
        "        test_sequences.append(data_split[i])\n",
        "    if i%4 == 3:\n",
        "        ground_truth_labels.append(data_split[i])\n",
        "\n",
        "# 인덱스 삭제\n",
        "del test_sequences[0]\n",
        "del ground_truth_labels[0]\n",
        "\n",
        "# 시퀀스 길이 정규화\n",
        "for i in range(0, len(test_sequences)):\n",
        "    while maxlen > len(test_sequences[i]):\n",
        "        test_sequences[i] += '0'\n",
        "\n",
        "# -------------------model--------------------\n",
        "\n",
        "mediandata_dir = \"median_data\"\n",
        "finaldata_dir = \"final_data\"\n",
        "\n",
        "predict10 = []\n",
        "prob10 = []\n",
        "\n",
        "for i in range(31) :\n",
        "\n",
        "    mymodel = NNC().to(device)\n",
        "    print(len(x))\n",
        "\n",
        "    learning_rate = 0.0015\n",
        "    batchsize = 400\n",
        "    epochs = 429\n",
        "\n",
        "    dataset = TensorDataset(x,y)\n",
        "    train_dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)\n",
        "\n",
        "    loss_func = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(mymodel.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t+1}\\n---------------------------\")\n",
        "        train_loop(train_dataloader, mymodel, loss_func, optimizer)\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------\n",
        "    # ----- test\n",
        "\n",
        "    # 모델 예측 수행\n",
        "    predictions, accuracy, probabilities, y_test = test_model(test_sequences, test_features_list, ground_truth_labels)\n",
        "\n",
        "    predict10.append(predictions)\n",
        "    prob10.append(probabilities)\n",
        "\n",
        "\n",
        "    # probabilities를 Tensor로 변환합니다.\n",
        "    probabilities = torch.tensor(probabilities)\n",
        "\n",
        "    # ROC curve를 계산합니다.\n",
        "    fpr, tpr, thresholds = roc_curve(y_test.cpu().numpy(), probabilities)\n",
        "\n",
        "    # AUC를 계산합니다.\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    # AUC 그래프를 그립니다.\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"test_sequences: \", test_sequences)\n",
        "    print(\"ground_truth_labels: \", ground_truth_labels)\n",
        "\n",
        "    # 예측 결과와 정확도 출력\n",
        "    print(\"Predictions: \", predictions)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"probabilities: \", probabilities)\n",
        "\n",
        "    # 텍스트 파일에 예측 결과 저장\n",
        "    file_path = f'/content/drive/My Drive/Testdata/{mediandata_dir}/test_result_nnc{i}.txt'  # 원하는 경로 및 파일명으로 변경 가능\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        for item in predictions:\n",
        "            file.write(item + '\\n')\n",
        "\n",
        "\n",
        "print(\"======================final======================\")\n",
        "from collections import Counter\n",
        "\n",
        "# 각 시퀀스에 대해 가장 많이 예측된 값을 최종 예측으로 사용합니다.\n",
        "final_predictions = [statistics.mode([predictions[i] for predictions in predict10]) for i in range(len(test_sequences))]\n",
        "\n",
        "# 최빈값\n",
        "mode_counts = [Counter([predictions[i] for predictions in predict10]).most_common(1)[0][1] for i in range(len(test_sequences))]\n",
        "print(mode_counts)\n",
        "\n",
        "# 최종 예측의 정확도를 계산합니다.\n",
        "final_accuracy = sum([1 if prediction == truth else 0 for prediction, truth in zip(final_predictions, ground_truth_labels)]) / len(ground_truth_labels)\n",
        "\n",
        "# 각 시퀀스에 대한 예측 확률의 평균을 계산합니다.\n",
        "average_probabilities = [statistics.mean([probabilities[i] for probabilities in prob10]) for i in range(len(test_sequences))]\n",
        "\n",
        "# ROC curve와 AUC를 계산합니다.\n",
        "fpr, tpr, thresholds = roc_curve(y_test.cpu().numpy(), average_probabilities)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(\"test_sequences: \", test_sequences)\n",
        "print(\"ground_truth_labels: \", ground_truth_labels)\n",
        "\n",
        "# 예측 결과와 정확도 출력\n",
        "print(\"Predictions: \", final_predictions)\n",
        "print(\"Accuracy: \", final_accuracy)\n",
        "print(\"probabilities: \", average_probabilities)\n",
        "\n",
        "file_path = f'/content/drive/My Drive/Testdata/{finaldata_dir}/final_result_nnc.txt'  # 원하는 경로 및 파일명으로 변경 가능\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    for item in final_predictions:\n",
        "        file.write(item + '\\n')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}